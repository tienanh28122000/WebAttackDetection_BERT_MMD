{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for training, you should modify these values to get the best performance\n",
    "config = {\n",
    "    \"num_labels\": 7,\n",
    "    \"hidden_dropout_prob\": 0.15,\n",
    "    \"hidden_size\": 768,\n",
    "    \"max_length\": 512,\n",
    "}\n",
    "\n",
    "training_parameters = {\n",
    "    \"batch_size\": 2,\n",
    "    \"epochs\": 1,\n",
    "    \"output_folder\": \"/kaggle/working\",\n",
    "    \"output_file\": \"model.bin\",\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"print_after_steps\": 5,\n",
    "    \"save_steps\": 5000,\n",
    "\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('jackaduma/SecBERT')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = self.df.iloc[index][\"text\"]\n",
    "        attack = self.df.iloc[index][\"label\"]\n",
    "        attack_dict = {'000 - Normal': 0,\n",
    "          '126 - Path Traversal': 1,\n",
    "          '242 - Code Injection': 2,\n",
    "          '153 - Input Data Manipulation': 3,\n",
    "          '310 - Scanning for Vulnerable Software': 4,\n",
    "          '194 - Fake the Source of Data': 5,\n",
    "          '34 - HTTP Response Splitting': 6}\n",
    "        label = attack_dict[attack]\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "                review,\n",
    "                add_special_tokens=True,\n",
    "                max_length= config[\"max_length\"],\n",
    "                pad_to_max_length=True,\n",
    "                return_overflowing_tokens=True,\n",
    "            )\n",
    "        if \"num_truncated_tokens\" in encoded_input and encoded_input[\"num_truncated_tokens\"] > 0:\n",
    "            # print(\"Attention! you are cropping tokens\")\n",
    "            pass\n",
    "\n",
    "        input_ids = encoded_input[\"input_ids\"]\n",
    "        attention_mask = encoded_input[\"attention_mask\"] if \"attention_mask\" in encoded_input else None\n",
    "\n",
    "        token_type_ids = encoded_input[\"token_type_ids\"] if \"token_type_ids\" in encoded_input else None\n",
    "\n",
    "\n",
    "\n",
    "        data_input = {\n",
    "            \"input_ids\": torch.tensor(input_ids),\n",
    "            \"attention_mask\": torch.tensor(attention_mask),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids),\n",
    "            \"label\": torch.tensor(label),\n",
    "        }\n",
    "\n",
    "        return data_input[\"input_ids\"], data_input[\"attention_mask\"], data_input[\"token_type_ids\"], data_input[\"label\"]\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for MMD implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "class GaussianKernel(nn.Module):\n",
    "    r\"\"\"Gaussian Kernel Matrix\n",
    "    Gaussian Kernel k is defined by\n",
    "    .. math::\n",
    "        k(x_1, x_2) = \\exp \\left( - \\dfrac{\\| x_1 - x_2 \\|^2}{2\\sigma^2} \\right)\n",
    "    where :math:`x_1, x_2 \\in R^d` are 1-d tensors.\n",
    "    Gaussian Kernel Matrix K is defined on input group :math:`X=(x_1, x_2, ..., x_m),`\n",
    "    .. math::\n",
    "        K(X)_{i,j} = k(x_i, x_j)\n",
    "    Also by default, during training this layer keeps running estimates of the\n",
    "    mean of L2 distances, which are then used to set hyperparameter  :math:`\\sigma`.\n",
    "    Mathematically, the estimation is :math:`\\sigma^2 = \\dfrac{\\alpha}{n^2}\\sum_{i,j} \\| x_i - x_j \\|^2`.\n",
    "    If :attr:`track_running_stats` is set to ``False``, this layer then does not\n",
    "    keep running estimates, and use a fixed :math:`\\sigma` instead.\n",
    "    Args:\n",
    "        sigma (float, optional): bandwidth :math:`\\sigma`. Default: None\n",
    "        track_running_stats (bool, optional): If ``True``, this module tracks the running mean of :math:`\\sigma^2`.\n",
    "          Otherwise, it won't track such statistics and always uses fix :math:`\\sigma^2`. Default: ``True``\n",
    "        alpha (float, optional): :math:`\\alpha` which decides the magnitude of :math:`\\sigma^2` when track_running_stats is set to ``True``\n",
    "    Inputs:\n",
    "        - X (tensor): input group :math:`X`\n",
    "    Shape:\n",
    "        - Inputs: :math:`(minibatch, F)` where F means the dimension of input features.\n",
    "        - Outputs: :math:`(minibatch, minibatch)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma: Optional[float] = None, track_running_stats: Optional[bool] = True,\n",
    "                 alpha: Optional[float] = 1.):\n",
    "        super(GaussianKernel, self).__init__()\n",
    "        assert track_running_stats or sigma is not None\n",
    "        self.sigma_square = torch.tensor(sigma * sigma) if sigma is not None else None\n",
    "        self.track_running_stats = track_running_stats\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        l2_distance_square = ((X.unsqueeze(0) - X.unsqueeze(1)) ** 2).sum(2)\n",
    "\n",
    "        if self.track_running_stats:\n",
    "            self.sigma_square = self.alpha * torch.mean(l2_distance_square.detach())\n",
    "\n",
    "        return torch.exp(-l2_distance_square / (2 * self.sigma_square))\n",
    "\n",
    "class MultipleKernelMaximumMeanDiscrepancy(nn.Module):\n",
    "    r\"\"\"The Multiple Kernel Maximum Mean Discrepancy (MK-MMD) used in\n",
    "    `Learning Transferable Features with Deep Adaptation Networks (ICML 2015) <https://arxiv.org/pdf/1502.02791>`_\n",
    "    Given source domain :math:`\\mathcal{D}_s` of :math:`n_s` labeled points and target domain :math:`\\mathcal{D}_t`\n",
    "    of :math:`n_t` unlabeled points drawn i.i.d. from P and Q respectively, the deep networks will generate\n",
    "    activations as :math:`\\{z_i^s\\}_{i=1}^{n_s}` and :math:`\\{z_i^t\\}_{i=1}^{n_t}`.\n",
    "    The MK-MMD :math:`D_k (P, Q)` between probability distributions P and Q is defined as\n",
    "    .. math::\n",
    "        D_k(P, Q) \\triangleq \\| E_p [\\phi(z^s)] - E_q [\\phi(z^t)] \\|^2_{\\mathcal{H}_k},\n",
    "    :math:`k` is a kernel function in the function space\n",
    "    .. math::\n",
    "        \\mathcal{K} \\triangleq \\{ k=\\sum_{u=1}^{m}\\beta_{u} k_{u} \\}\n",
    "    where :math:`k_{u}` is a single kernel.\n",
    "    Using kernel trick, MK-MMD can be computed as\n",
    "    .. math::\n",
    "        \\hat{D}_k(P, Q) &=\n",
    "        \\dfrac{1}{n_s^2} \\sum_{i=1}^{n_s}\\sum_{j=1}^{n_s} k(z_i^{s}, z_j^{s})\\\\\n",
    "        &+ \\dfrac{1}{n_t^2} \\sum_{i=1}^{n_t}\\sum_{j=1}^{n_t} k(z_i^{t}, z_j^{t})\\\\\n",
    "        &- \\dfrac{2}{n_s n_t} \\sum_{i=1}^{n_s}\\sum_{j=1}^{n_t} k(z_i^{s}, z_j^{t}).\\\\\n",
    "    Args:\n",
    "        kernels (tuple(torch.nn.Module)): kernel functions.\n",
    "        linear (bool): whether use the linear version of DAN. Default: False\n",
    "    Inputs:\n",
    "        - z_s (tensor): activations from the source domain, :math:`z^s`\n",
    "        - z_t (tensor): activations from the target domain, :math:`z^t`\n",
    "    Shape:\n",
    "        - Inputs: :math:`(minibatch, *)`  where * means any dimension\n",
    "        - Outputs: scalar\n",
    "    .. note::\n",
    "        Activations :math:`z^{s}` and :math:`z^{t}` must have the same shape.\n",
    "    .. note::\n",
    "        The kernel values will add up when there are multiple kernels.\n",
    "    Examples::\n",
    "        >>> from tllib.modules.kernels import GaussianKernel\n",
    "        >>> feature_dim = 1024\n",
    "        >>> batch_size = 10\n",
    "        >>> kernels = (GaussianKernel(alpha=0.5), GaussianKernel(alpha=1.), GaussianKernel(alpha=2.))\n",
    "        >>> loss = MultipleKernelMaximumMeanDiscrepancy(kernels)\n",
    "        >>> # features from source domain and target domain\n",
    "        >>> z_s, z_t = torch.randn(batch_size, feature_dim), torch.randn(batch_size, feature_dim)\n",
    "        >>> output = loss(z_s, z_t)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernels: Sequence[nn.Module], linear: Optional[bool] = False):\n",
    "        super(MultipleKernelMaximumMeanDiscrepancy, self).__init__()\n",
    "        self.kernels = kernels\n",
    "        self.index_matrix = None\n",
    "        self.linear = linear\n",
    "\n",
    "    def forward(self, z_s: torch.Tensor, z_t: torch.Tensor) -> torch.Tensor:\n",
    "        features = torch.cat([z_s, z_t], dim=0)\n",
    "        batch_size = int(z_s.size(0))\n",
    "        self.index_matrix = _update_index_matrix(batch_size, self.index_matrix, self.linear).to(z_s.device)\n",
    "\n",
    "\n",
    "        kernel_matrix = sum([kernel(features) for kernel in self.kernels])  # Add up the matrix of each kernel\n",
    "        # Add 2 / (n-1) to make up for the value on the diagonal\n",
    "        # to ensure loss is positive in the non-linear version\n",
    "        loss = (kernel_matrix * self.index_matrix).sum() + 2. / float(batch_size - 1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def _update_index_matrix(batch_size: int, index_matrix: Optional[torch.Tensor] = None,\n",
    "                         linear: Optional[bool] = True) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Update the `index_matrix` which convert `kernel_matrix` to loss.\n",
    "    If `index_matrix` is a tensor with shape (2 x batch_size, 2 x batch_size), then return `index_matrix`.\n",
    "    Else return a new tensor with shape (2 x batch_size, 2 x batch_size).\n",
    "    \"\"\"\n",
    "    if index_matrix is None or index_matrix.size(0) != batch_size * 2:\n",
    "        index_matrix = torch.zeros(2 * batch_size, 2 * batch_size)\n",
    "        if linear:\n",
    "            for i in range(batch_size):\n",
    "                s1, s2 = i, (i + 1) % batch_size\n",
    "                t1, t2 = s1 + batch_size, s2 + batch_size\n",
    "                index_matrix[s1, s2] = 1. / float(batch_size)\n",
    "                index_matrix[t1, t2] = 1. / float(batch_size)\n",
    "                index_matrix[s1, t2] = -1. / float(batch_size)\n",
    "                index_matrix[s2, t1] = -1. / float(batch_size)\n",
    "        else:\n",
    "            for i in range(batch_size):\n",
    "                for j in range(batch_size):\n",
    "                    if i != j:\n",
    "                        index_matrix[i][j] = 1. / float(batch_size * (batch_size - 1))\n",
    "                        index_matrix[i + batch_size][j + batch_size] = 1. / float(batch_size * (batch_size - 1))\n",
    "            for i in range(batch_size):\n",
    "                for j in range(batch_size):\n",
    "                    index_matrix[i][j + batch_size] = -1. / float(batch_size * batch_size)\n",
    "                    index_matrix[i + batch_size][j] = -1. / float(batch_size * batch_size)\n",
    "    return index_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset include source dataset and target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('training.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional (not effect very much)\n",
    "# for word tokenizer instead of character tokenizer\n",
    "df_train['text'] = df_train['text'].str.replace('/',' ')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare for training\n",
    "df_train = df_train[0:len(df_train)//training_parameters['batch_size']*training_parameters['batch_size']]\n",
    "source_dataset = ReviewDataset(df_train)\n",
    "source_dataloader = DataLoader(dataset = source_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfer = pd.read_csv('transfer.csv')\n",
    "df_transfer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional (not effect very much)\n",
    "# for word tokenizer instead of character tokenizer\n",
    "df_transfer['text'] = df_transfer['text'].str.replace('/',' ')\n",
    "df_transfer = df_transfer[0:len(df_transfer)//training_parameters['batch_size']*training_parameters['batch_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dataset = ReviewDataset(df_transfer)\n",
    "target_dataloader = DataLoader(dataset = target_dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class DomainAdaptationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DomainAdaptationModel, self).__init__()\n",
    "        \n",
    "        num_labels = config[\"num_labels\"]\n",
    "        self.bert = AutoModel.from_pretrained('jackaduma/SecBERT') # model that we will use\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "        \n",
    "        self.prj = nn.Linear(config[\"hidden_size\"], config[\"hidden_size\"]//2);\n",
    "        \n",
    "        self.attack_classifier = nn.Sequential(\n",
    "            nn.Linear(config[\"hidden_size\"]//2, num_labels),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "\n",
    "#       Freeze bert layer\n",
    "        modules = [self.bert.embeddings, self.bert.encoder.layer[:2]] #Replace value by what you want\n",
    "        for module in modules:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(\n",
    "          self,\n",
    "          input_ids=None,\n",
    "          attention_mask=None,\n",
    "          token_type_ids=None,\n",
    "          labels=None,\n",
    "#           grl_lambda = 1.0, \n",
    "          ):\n",
    "\n",
    "        outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "            )\n",
    "\n",
    "#         pooled_output = outputs[1] # For bert-base-uncase\n",
    "        pooled_output = outputs.pooler_output \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        pooled_output_prj = self.prj(pooled_output)\n",
    "\n",
    "        attack_pred = self.attack_classifier(pooled_output_prj)\n",
    "\n",
    "        return attack_pred.to(device), pooled_output_prj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(logits, labels):\n",
    "    \n",
    "    predicted_labels_dict = {\n",
    "      0: 0,\n",
    "      1: 0,\n",
    "      2: 0,\n",
    "      3: 0,\n",
    "      4: 0,\n",
    "      5: 0,\n",
    "      6: 0,\n",
    "    }\n",
    "    \n",
    "    predicted_label = logits.max(dim = 1)[1]\n",
    "    \n",
    "    for pred in predicted_label:\n",
    "        # print(pred.item())\n",
    "        predicted_labels_dict[pred.item()] += 1\n",
    "    acc = (predicted_label == labels).float().mean()\n",
    "    \n",
    "    return acc, predicted_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataset = \"target\", percentage = 80):\n",
    "    with torch.no_grad():\n",
    "        predicted_labels_dict = {                                                   \n",
    "          0: 0,\n",
    "          1: 0,\n",
    "          2: 0,\n",
    "          3: 0,\n",
    "          4: 0,\n",
    "          5: 0,\n",
    "          6: 0,                                                                   \n",
    "        }\n",
    "        \n",
    "        dev_df = pd.read_csv(\"/kaggle/input/code-injection/dataset_capec_\" + dataset + \".csv\")\n",
    "        data_size = dev_df.shape[0]\n",
    "        selected_for_evaluation = int(data_size*percentage/100)\n",
    "        dev_df = dev_df.head(selected_for_evaluation)\n",
    "        dataset = ReviewDataset(dev_df)\n",
    "\n",
    "        dataloader = DataLoader(dataset = dataset, batch_size = training_parameters[\"batch_size\"], shuffle = True, num_workers = 2)\n",
    "\n",
    "        mean_accuracy = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "        \n",
    "        for input_ids, attention_mask, token_type_ids, labels in dataloader:\n",
    "            inputs = {\n",
    "                \"input_ids\": input_ids.squeeze(axis=1),\n",
    "                \"attention_mask\": attention_mask.squeeze(axis=1),\n",
    "                \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n",
    "                \"labels\": labels,\n",
    "            }\n",
    "            for k, v in inputs.items():\n",
    "                inputs[k] = v.to(device)\n",
    "\n",
    "\n",
    "            attack_pred, _ = model(**inputs)\n",
    "            accuracy, predicted_labels = compute_accuracy(attack_pred, inputs[\"labels\"])\n",
    "            mean_accuracy += accuracy\n",
    "            for i in range(7): \n",
    "              predicted_labels_dict[i] += predicted_labels[i]\n",
    "\n",
    "        print(predicted_labels_dict)\n",
    "    return mean_accuracy/total_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = training_parameters[\"learning_rate\"]\n",
    "n_epochs = training_parameters[\"epochs\"]\n",
    "\n",
    "model = DomainAdaptationModel()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "loss_fn_attack_classifier = torch.nn.NLLLoss()\n",
    "# loss_fn_domain_classifier = torch.nn.NLLLoss()\n",
    "mkmmd_loss = MultipleKernelMaximumMeanDiscrepancy(\n",
    "        kernels=[GaussianKernel(alpha=2 ** k) for k in range(-3, 2)],\n",
    "        linear=True\n",
    "    )\n",
    "'''\n",
    "In one training step we will update the model using both the source labeled data and target unlabeled data\n",
    "We will run it till the batches last for any of these datasets\n",
    "\n",
    "In our case target dataset has more data. Hence, we will leverage the entire source dataset for training\n",
    "\n",
    "If we use the same approach in a case where the source dataset has more data then the target dataset then we will\n",
    "under-utilize the labeled source dataset. In such a scenario it is better to reload the target dataset when it finishes\n",
    "This will ensure that we are utilizing the entire source dataset to train our model.\n",
    "'''\n",
    "\n",
    "max_batches = min(len(source_dataloader), len(target_dataloader))\n",
    "\n",
    "for epoch_idx in range(n_epochs):\n",
    "    \n",
    "    source_iterator = iter(source_dataloader)\n",
    "    target_iterator = iter(target_dataloader)\n",
    "\n",
    "    for batch_idx in range(max_batches):\n",
    "        \n",
    "        p = float(batch_idx + epoch_idx * max_batches) / (training_parameters[\"epochs\"] * max_batches)\n",
    "        grl_lambda = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "        grl_lambda = torch.tensor(grl_lambda)\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        if(batch_idx%training_parameters[\"print_after_steps\"] == 0 ):\n",
    "            print(\"Training Step:\", batch_idx)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Souce dataset training update\n",
    "        input_ids, attention_mask, token_type_ids, labels = next(source_iterator)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids.squeeze(axis=1),\n",
    "            \"attention_mask\": attention_mask.squeeze(axis=1),\n",
    "            \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n",
    "            \"labels\" : labels,\n",
    "        }\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "    \n",
    "        attack_pred, pooled_output_prj_source = model(**inputs)\n",
    "        loss_s_attack = loss_fn_attack_classifier(attack_pred, inputs[\"labels\"])\n",
    "\n",
    "\n",
    "        # Target dataset training update \n",
    "        input_ids, attention_mask, token_type_ids, labels = next(target_iterator)\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids.squeeze(axis=1),\n",
    "            \"attention_mask\": attention_mask.squeeze(axis=1),\n",
    "            \"token_type_ids\" : token_type_ids.squeeze(axis=1),\n",
    "            \"labels\" : labels,\n",
    "        }\n",
    "\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "    \n",
    "        _, pooled_output_prj_target = model(**inputs)\n",
    "        \n",
    "\n",
    "        # Combining the loss \n",
    "\n",
    "        transfer_loss = mkmmd_loss(pooled_output_prj_source, pooled_output_prj_target);\n",
    "        loss = loss_s_attack + transfer_loss*1.0\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.join(training_parameters[\"output_folder\"], \"epoch_\" + str(epoch_idx)  +  training_parameters[\"output_file\"] ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get accuracy on source and target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(model, dataset = \"source\", percentage = 100).item()\n",
    "print(\"Accuracy on source dataset after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(model, dataset = \"target\", percentage = 100).item()\n",
    "print(\"Accuracy on target dataset after epoch \" + str(epoch_idx) + \" is \" + str(accuracy))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
